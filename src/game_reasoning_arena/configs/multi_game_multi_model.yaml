# Config for running multiple games with multiple LLM models
# Example command to run:
# python3 scripts/run_multi_model_games.py --config src/game_reasoning_arena/configs/multi_game_multi_model.yaml

# N.B. This config takes a while to run, there is a different script to run the same with Ray

# Use env_configs (plural) for multiple games as a list
env_configs:
  - game_name: kuhn_poker
    max_game_rounds: null
  - game_name: connect_four
    max_game_rounds: null
  - game_name: tic_tac_toe
    max_game_rounds: null
  - game_name: matrix_rps
    max_game_rounds: 10
  - game_name: matrix_pd
    max_game_rounds: 10
  - game_name: matching_pennies
    max_game_rounds: 10
  # - game_name: hex  #  takes too much time, run standalone
  #   max_game_rounds: 50      # Limit HEX games to prevent infinite loops

num_episodes: 1
seed: 42
use_ray: false
parallel_episodes: true     # Parallelize episodes within games
mode: llm_vs_random

# To run all models, override agents.player_0.model in the script or loop over these models externally
models:
  # - litellm_groq/llama3-8b-8192  HAS BEEN DECOMMISIONED Aug 30   # Fast, good performance
  - litellm_groq/llama-3.1-8b-instant  # new models
#  - litellm_groq/llama-3.1-70b-versatile # new models - it doesnt work
  - litellm_groq/gemma2-9b-it        # Fast, reliable
  - litellm_groq/llama3-70b-8192   # SLOW - commented out for HEX
  - litellm_gpt-3.5-turbo
  - litellm_gpt-4
  - litellm_gpt-4-turbo
  - litellm_gpt-o4-mini
#  - litellm_gpt-5  # Not working well - need fix
#  - litellm_gpt-5-mini  # Not working well - need fix
  - litellm_together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
  - litellm_together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
  - litellm_together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1
  - litellm_fireworks_ai/accounts/fireworks/models/llama-v3-8b-instruct
  - litellm_fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct
  - litellm_fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-thinking-2507
  - litellm_fireworks_ai/accounts/fireworks/models/glm-4p5-air
  - litellm_fireworks_ai/accounts/fireworks/models/kimi-k2-instruct



agents:
  player_0:
    type: llm
    model: litellm_groq/llama3-8b-8192  # Default (will be overridden per model)
  player_1:
    type: random

llm_backend:
  max_tokens: 150           # Reduced for faster responses
  temperature: 0.1
  default_model: litellm_groq/llama3-8b-8192

log_level: WARNING          # Reduced logging for speed
tensorboard_logging: false # Disabled by default for performance
