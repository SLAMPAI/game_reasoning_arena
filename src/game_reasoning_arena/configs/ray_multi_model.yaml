# Ray-Optimized Multi-Model Configuration
# This config is designed for maximum parallel throughput using Ray
#
# Usage:
# python3 scripts/run_ray_multi_model.py --config src/game_reasoning_arena/configs/ray_multi_model.yaml

# Games to run (including longer games with Ray parallelization)
# Use env_configs (plural) for multiple games as a list
env_configs:
  - game_name: tic_tac_toe
    max_game_rounds: null
  - game_name: kuhn_poker
    max_game_rounds: null
  - game_name: matrix_rps
    max_game_rounds: 10
  - game_name: matrix_pd
    max_game_rounds: 10
  - game_name: matching_pennies
    max_game_rounds: 10
  - game_name: connect_four
    max_game_rounds: null
  - game_name: hex
    max_game_rounds: 50      # Limited rounds to prevent infinite games

# Parallel execution settings
num_episodes: 5              # More episodes for better statistics
seed: 42
use_ray: true               # Enable Ray distributed computing
parallel_episodes: true     # Parallelize episodes within games
mode: llm_vs_random

# Models to test (carefully selected for speed/performance balance)
models:
 # - litellm_groq/llama3-8b-8192  HAS BEEN DECOMMISIONED Aug 30   # Fast, good performance
  - litellm_groq/llama-3.1-8b-instant  # new models
  - litellm_groq/llama-3.1-70b-versatile # new models
  - litellm_groq/gemma2-9b-it        # Fast, reliable
  - litellm_groq/llama3-70b-8192   # SLOW - commented out for HEX
  - litellm_gpt-3.5-turbo
  - litellm_gpt-4
  - litellm_gpt-4-turbo
  - litellm_gpt-o4-mini
#  - litellm_gpt-5  # Not working well - need fix
#  - litellm_gpt-5-mini  # Not working well - need fix
  - litellm_together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
  - litellm_together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
  - litellm_together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1
  - litellm_fireworks_ai/accounts/fireworks/models/llama-v3-8b-instruct
  - litellm_fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct
  - litellm_fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-thinking-2507
  - litellm_fireworks_ai/accounts/fireworks/models/glm-4p5-air
  - litellm_fireworks_ai/accounts/fireworks/models/kimi-k2-instruct

# Agent configuration
agents:
  player_0:
    type: llm
    model: litellm_groq/llama3-8b-8192  # Default (will be overridden per model)
  player_1:
    type: random

# Ray configuration optimized for multi-model parallel execution
ray_config:
  # num_cpus: auto-detected by Ray (recommended)
  # object_store_memory: auto-detected by Ray (recommended)
  include_dashboard: false            # Disable for headless operation

# LLM settings optimized for speed
llm_backend:
  max_tokens: 100                     # Reduced for faster responses
  temperature: 0.1                    # Low temperature for consistency
  default_model: litellm_groq/llama3-8b-8192

# Logging optimized for parallel execution
log_level: ERROR                      # Minimal logging to reduce overhead
tensorboard_logging: false           # Disabled for performance during parallel runs
