\begin{table}[htbp]
\centering
\caption{Overall Model Performance Across All Games}
\begin{tabular}{lcccc}
\toprule
Model & Games Played & Games Won & Win Rate (\%) & Avg Reward \\
\midrule
GPT-3.5-turbo & 5 & 5 & 100.00 & 1.800 ± 0.360 \\
Meta-Llama-3.1-70B-Instruct-Turbo & 10 & 10 & 100.00 & 2.800 ± 0.560 \\
Gemma-7b-it & 1 & 1 & 100.00 & 1.000 ± 0.200 \\
llama-3.1-8b-instant & 16 & 14 & 87.50 & 0.750 ± 0.150 \\
Llama-3-70b-8192 & 54 & 47 & 87.04 & 2.148 ± 0.430 \\
Qwen3-32b & 13 & 11 & 84.62 & 1.462 ± 0.292 \\
qwen3-235b-a22b-thinking-2507 & 18 & 15 & 83.33 & 1.444 ± 0.289 \\
Llama-3-8b-8192 & 166 & 138 & 83.13 & 1.012 ± 0.202 \\
kimi-k2-instruct & 16 & 13 & 81.25 & 2.438 ± 0.488 \\
Meta-Llama-3.1-8B-Instruct-Turbo & 10 & 8 & 80.00 & 1.500 ± 0.300 \\
llama-v3-8b-instruct & 10 & 8 & 80.00 & 1.600 ± 0.320 \\
llama-v3-70b-instruct & 10 & 8 & 80.00 & 1.600 ± 0.320 \\
GPT-4-turbo & 5 & 4 & 80.00 & 2.400 ± 0.480 \\
GPT-4-mini & 32 & 24 & 75.00 & 1.281 ± 0.256 \\
GPT-4 & 44 & 32 & 72.73 & 1.841 ± 0.368 \\
o4-mini & 11 & 8 & 72.73 & 1.455 ± 0.291 \\
Gemma2-9b-it & 58 & 39 & 67.24 & 1.414 ± 0.283 \\
glm-4p5-air & 50 & 32 & 64.00 & 1.640 ± 0.328 \\
ai-mistralai-Mixtral-8x7B-Instruct-v0.1 & 59 & 35 & 59.32 & 0.966 ± 0.193 \\
llama-3.1-8b-instant & 0 & 0 & 0.00 & 0.000 ± 0.000 \\
Qwen2-7B-Instruct & 1 & 0 & 0.00 & -1.000 ± -0.200 \\
\bottomrule
\end{tabular}
\end{table}
